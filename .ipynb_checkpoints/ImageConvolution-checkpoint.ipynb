{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70a5ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    import requests\n",
    "    url = 'https://raw.githubusercontent.com/Ishank-dubey/unSupervisedLearning/master/config.py'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open('config.py', 'wb').write(r.content)\n",
    "except ModuleNotFoundError:\n",
    "    pass\n",
    "\n",
    "from config import *\n",
    "download_to_colab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4713b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "<class 'torch.utils.data.dataset.TensorDataset'>\n",
      "OrderedDict([('0.weight', tensor([[1.9416]])), ('0.bias', tensor([1.0235]))])\n",
      "200\n",
      "[[1.9942763]\n",
      " [1.605955 ]\n",
      " [2.3825974]]\n",
      "OrderedDict([('0.weight', tensor([[0.7645]])), ('0.bias', tensor([0.8300]))])\n",
      "OrderedDict([('0.weight', tensor([[1.9416]])), ('0.bias', tensor([1.0235]))])\n",
      "OrderedDict([('0.weight', tensor([[1.9416]])), ('0.bias', tensor([1.0235]))])\n",
      "(1, 1, 6, 6)\n",
      "(1, 1, 3, 3)\n",
      "9\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import Compose, Normalize\n",
    "\n",
    "from data_generation.image_classification import generate_dataset\n",
    "from helpers import index_splitter, make_balanced_sampler\n",
    "from StepByStep import StepByStep\n",
    "\n",
    "\n",
    "#'Convolution'\n",
    "single = np.array(\n",
    "    [[[[5, 0, 8, 7, 8, 1],\n",
    "       [1, 9, 5, 0, 7, 7],\n",
    "       [6, 0, 2, 4, 6, 6],\n",
    "       [9, 7, 6, 6, 8, 4],\n",
    "       [8, 3, 8, 5, 1, 3],\n",
    "       [7, 2, 7, 0, 1, 0]]]]\n",
    ")\n",
    "print(single.shape)\n",
    "\n",
    "identity = np.array(\n",
    "    [[[[0, 0, 0],\n",
    "       [0, 1, 0],\n",
    "       [0, 0, 0]]]]\n",
    ")\n",
    "print(identity.shape)\n",
    "\n",
    "region = single[:, :, 0:3, 0:3]\n",
    "filtered_region = region * identity\n",
    "total = filtered_region.sum()\n",
    "print(total)\n",
    "\n",
    "new_region = single[:, :, 0:3, (0+1):(3+1)]\n",
    "new_region_filter = new_region * identity\n",
    "print(new_region_filter.sum())\n",
    "\n",
    "# given an Image of hi x wi and filter hf x wf\n",
    "# the convolution will result in the image iof size - (hi + 1 - hf, wi +1 - wf)\n",
    "# this size is lesser than the image so\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0692a45e",
   "metadata": {},
   "source": [
    "# Convolving in PyTorch\n",
    "\n",
    "#### Filter/Kernels\n",
    "![](./images/conv1.png)\n",
    "\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "(h_i, w_i) * f = (h_i - f + 1, w_i - f + 1)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0599f9cc",
   "metadata": {},
   "source": [
    "##### Use padding with zero if want to keep the image size same as the input image\n",
    "##### In torch conv can be a function and also a learnable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "615f7dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[9., 5., 0., 7.],\n",
       "          [0., 2., 4., 6.],\n",
       "          [7., 6., 6., 8.],\n",
       "          [3., 8., 5., 1.]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Simple function\n",
    "original_image = torch.as_tensor(single).float()\n",
    "kernel = torch.as_tensor(identity).float()\n",
    "covolution_result = F.conv2d(original_image, kernel,  stride=1)\n",
    "covolution_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5ac1ee",
   "metadata": {},
   "source": [
    "###### torch's nn.Conv2d will only input the in_channel out_channel, kernel size and stride, note that it wont ask for the parameters, they would be learnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c64dba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.0728, -4.4207, -0.7492, -1.2183],\n",
       "          [-2.8472, -0.7523, -1.6457, -2.4167],\n",
       "          [-3.5240, -1.9131, -2.0134, -3.9783],\n",
       "          [-2.4814, -1.9108, -4.6542, -1.0758]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, stride=1)\n",
    "conv(original_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0abf367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there can bo more than one channel based on the out_channels like\n",
    "conv_multiple = nn.Conv2d()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
